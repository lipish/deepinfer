# Default Configuration for DeepInfer
# This is a general-purpose configuration that works on most GPUs

model:
  name: "meta-llama/Llama-3.1-8B-Instruct"
  trust_remote_code: false
  tokenizer_mode: "auto"
  dtype: "auto"
  max_model_len: null
  quantization: null

gpu:
  device: "cuda"
  gpu_memory_utilization: 0.90
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_num_batched_tokens: null
  max_num_seqs: 256
  max_paddings: 256
  enable_chunked_prefill: false
  enable_prefix_caching: true
  disable_log_stats: false
  nvidia_5090_optimizations: false
  kv_cache_dtype: "auto"
  enforce_eager: false

server:
  host: "0.0.0.0"
  port: 8000
  log_level: "info"
  api_key: null
  allowed_origins:
    - "*"
  timeout: 600

sampling:
  temperature: 0.7
  top_p: 0.95
  top_k: -1
  max_tokens: 512
  presence_penalty: 0.0
  frequency_penalty: 0.0
  repetition_penalty: 1.0
  stop: null
