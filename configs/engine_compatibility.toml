# Engine-Device Compatibility Matrix
#
# Declarative configuration for which engines support which devices and data types

[engines.vllm]
name = "vLLM"
version = "0.6.0+"
supported_devices = ["cuda", "rocm", "cpu"]
supported_formats = ["fp32", "fp16", "bf16", "fp8", "fp4", "int8", "int4"]

# Device-specific capabilities
[engines.vllm.device_capabilities]

[engines.vllm.device_capabilities.cuda]
min_compute_capability = [7, 0]  # Volta and newer
recommended_compute_capability = [8, 0]  # Ampere for best performance
blackwell_support = true  # SM 10.0 / RTX 5090
max_tensor_parallel = 8

[engines.vllm.device_capabilities.cuda.dtype_support]
# Compute capability -> supported dtypes
"7.0" = ["fp32", "fp16", "int8"]  # Volta
"8.0" = ["fp32", "fp16", "bf16", "int8"]  # Ampere  
"9.0" = ["fp32", "fp16", "bf16", "fp8", "int8"]  # Hopper
"10.0" = ["fp32", "fp16", "bf16", "fp8", "fp4", "int8", "int4"]  # Blackwell (RTX 5090)

[engines.vllm.device_capabilities.rocm]
min_version = "5.7"
supported_gpus = ["MI250", "MI300"]

[engines.vllm.device_capabilities.cpu]
# CPU fallback with limited performance
recommended = false
max_batch_size = 16

# Future engines
[engines.tgi]
name = "Text Generation Inference"
supported_devices = ["cuda"]
# TODO: Add TGI configuration

[engines.tensorrt_llm]
name = "TensorRT-LLM"
supported_devices = ["cuda"]
min_compute_capability = [8, 0]
# TODO: Add TensorRT-LLM configuration
